{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 (Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**토큰화 목적**\n",
    "- 문법적 구조 이해\n",
    "- 유연한 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP is fascinating. It has many applications in real-world scenarios.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinating', '.', 'It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']\n",
      "['NLP is fascinating.', 'It has many applications in real-world scenarios.']\n",
      "['NLP', 'is', 'fascinating', '.']\n",
      "['It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 단어 토큰화\n",
    "print(nltk.word_tokenize(text))\n",
    "\n",
    "# 문장 토큰화\n",
    "print(nltk.sent_tokenize(text))\n",
    "\n",
    "# 문장별 단어 토큰화\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    print(nltk.word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BertTokenizer\n",
    "    - 단어를 부분 단위로 쪼개어 희귀하거나 새로운 단어도 부분적으로 표현할 수 있도록 함 \n",
    "    => 어휘 크기를 줄이고 다양한 언어 패턴 학습 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['un', '##ha', '##pp', '##iness']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 가장 작은 단위로 쪼갬\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "word = 'unhappiness'\n",
    "subwords = tokenizer.tokenize(word)\n",
    "subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 분리 과정\n",
    "1. WordPiece 토크나이저는 사전에 등록된 단어 우선 사용\n",
    "    - Bert의 WordPiece 토크나이저는 최대한 긴 단어를 먼저 찾으려는 방식으로 동작\n",
    "    - 'Happy'가 사전에 있으면 그대로 사용, 없으면 가장 긴 서브워드 조합 찾아 나눔\n",
    "2. 'happy'가 사전에 포함되지 않은 경우\n",
    "    - 일반적으로 'Happy'는 BERT의 사전에 포함되어 있지만, 학습된 모델에 없을 수 있음\n",
    "    - 'happy' 가 사전에 없는 경우 사전에 있는 조각(서브워드)로 분해\n",
    "3. 그래서 과정은...\n",
    "    - \"unhapiness\" 검색 -> 없음\n",
    "    - \"un\" 검색 -> 있음 -> 유지\n",
    "    - \"happiness\" 검색 -> 없음\n",
    "    - \"ha\" 검색 -> 있음\n",
    "    - \"pp\" 검색 -> 있음\n",
    "    - \"iness\" 검색 -> 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nl',\n",
       " '##p',\n",
       " 'is',\n",
       " 'fascinating',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'many',\n",
       " 'applications',\n",
       " 'in',\n",
       " 'real',\n",
       " '-',\n",
       " 'world',\n",
       " 'scenarios',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자 단위 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화 주의사항\n",
    "1. 구두점이나 특수 문자를 단순 제외하면 안됨\n",
    "2. 줄임말, 단어 내 띄어쓰기 유의\n",
    "\n",
    "**표준 토큰화 예제 *Penn Treebank Tokenization**\n",
    "- 규칙 1: 하이픈으로 구성된 단어는 하나로 유지한다.\n",
    "- 규칙 2: doesn't와 같이 '가 있는 단어는 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'an',\n",
       " 'arrow',\n",
       " 'fruit',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'a',\n",
       " 'banana']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 구두점을 제외한 단어 토큰화\n",
    "import re\n",
    "\n",
    "text = \"Time flies like an arrow; fruit flies like a banana.\"\n",
    "re.findall(r'\\b\\w+\\b', text)    # r -> raw\n",
    "# r'' : 파이썬 raw text (이스케이핑 문자를 문자 그대로 처리)\n",
    "# \\b : 경계 문자 (boundary / 공백, 구두점, ...)\n",
    "# \\w : 글자 (word / 영문자, 숫자, _)\n",
    "# \\w+ : 수랑자 (하나 이상)\n",
    "# => 경계 문자로 감싸진 하나 이상의 글자를 전부 찾아라~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'use', 'well', '-', 'being', 'practices', 'for', 'self', '-', 'care', '.']\n",
      "['Do', \"n't\", 'hesitate', 'to', 'use', 'well-being', 'practices', 'for', 'self-care', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer\n",
    "# 단어/구두점으로 토큰을 구분 (', - 포함 단어도 분리)\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "text = \"Don't hesitate to use well-being practices for self-care.\"\n",
    "print(word_punct_tokenizer.tokenize(text))\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COVID-19',\n",
       " '(',\n",
       " '전염병',\n",
       " ')',\n",
       " ',',\n",
       " 'Dr.Smith',\n",
       " '(',\n",
       " '의사',\n",
       " ')',\n",
       " ',',\n",
       " 'NASA',\n",
       " '(',\n",
       " '우주항공국',\n",
       " ')',\n",
       " ',',\n",
       " '등',\n",
       " '특정',\n",
       " '기관이나',\n",
       " '명칭이',\n",
       " '있다.',\n",
       " '특수',\n",
       " '문자',\n",
       " '또한',\n",
       " '태그',\n",
       " '<',\n",
       " 'br',\n",
       " '>',\n",
       " ',',\n",
       " '가격',\n",
       " '$',\n",
       " '100.50',\n",
       " ',',\n",
       " '2025/02/18',\n",
       " '날짜',\n",
       " '표현에',\n",
       " '사용될',\n",
       " '수',\n",
       " '있다.',\n",
       " '이러한',\n",
       " '경우',\n",
       " ',',\n",
       " '$',\n",
       " '100.50을',\n",
       " '하나의',\n",
       " '토큰으로',\n",
       " '유지할',\n",
       " '필요가',\n",
       " '있다',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "Treebank_word_tokenizer = TreebankWordTokenizer()\n",
    "text = '''\n",
    "COVID-19(전염병), Dr.Smith(의사), NASA(우주항공국), 등 특정 기관이나 명칭이 있다.\n",
    "특수 문자 또한 태그 <br>, 가격 $100.50, 2025/02/18 날짜 표현에 사용될 수 있다.\n",
    "이러한 경우, $100.50을 하나의 토큰으로 유지할 필요가 있다.\n",
    "'''\n",
    "\n",
    "Treebank_word_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install kss==5.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://github.com/hyunwoongko/python-mecab-kor\n",
      "- konlpy.tag.Mecab: https://konlpy.org/en/latest/api/konlpy.tag/#mecab-class\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['배경은 1920년대의 경성부이다.',\n",
       " '주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다.',\n",
       " \"아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문.\",\n",
       " '멍청이, 꼰대...가 아닐수 없다.',\n",
       " '사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = \"배경은 1920년대의 경성부이다. 주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다. 아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다.'는 김 첨지의 신조 때문. 멍청이, 꼰대...가 아닐수 없다. 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.\"\n",
    "\n",
    "kss.split_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/isejin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/isejin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('arrow', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사별로 나누는 방법\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = 'Time flies like an arrow.'\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.10/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.10/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from spacy) (75.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/isejin/Library/Python/3.10/lib/python/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/isejin/Library/Python/3.10/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# spacy.cli.download('en_core_web_sm')     # 직접 다운로드 (사용 전 1회는 반드시 다운.)\n",
    "spacy_nlp = spacy.load('en_core_web_sm') # 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : NOUN\n",
      "flies : VERB\n",
      "like : ADP\n",
      "an : DET\n",
      "arrow : NOUN\n",
      ". : PUNCT\n"
     ]
    }
   ],
   "source": [
    "tokens = spacy_nlp(text)\n",
    "for token in tokens:\n",
    "    print(token.text, \":\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "- 한국어 자연어 처리를 위한 라이브러리\n",
    "- 형태소 분석, 품사 태깅, 텍스트 전처리 등 기능 지원\n",
    "- 여러 형태소 분석기 중 적합한 분석기 선택 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/site-packages (from konlpy) (1.5.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/site-packages (from konlpy) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/isejin/Library/Python/3.10/lib/python/site-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '버스', '를', '기다리고', '있다', '.', '지각', '은', '면', '하겠군', '!']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "text = \"나는 버스를 기다리고 있다. 지각은 면하겠군!\"\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석\n",
    "morphs = okt.morphs(text)\n",
    "morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('나', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('버스', 'Noun'),\n",
       " ('를', 'Josa'),\n",
       " ('기다리고', 'Verb'),\n",
       " ('있다', 'Adjective'),\n",
       " ('.', 'Punctuation'),\n",
       " ('지각', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('면', 'Noun'),\n",
       " ('하겠군', 'Verb'),\n",
       " ('!', 'Punctuation')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = okt.pos(text)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '버스', '지각', '면']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = okt.nouns(text)\n",
    "nouns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
