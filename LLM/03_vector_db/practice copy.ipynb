{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vector db 활용 학습\n",
    "- 주제: @@@ 감성 분석 or 사용자 맞춤 데이터 출력\n",
    "1. 적절한 크기의 데이터셋을 구한다.\n",
    "2. 한국어 임베딩에 적절한 모델을 찾는다.\n",
    "3. Chroma DB, FAISS, Pinecone 중 원하는 벡터 DB를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv('./data/train.tsv', sep='\\t')\n",
    "\n",
    "# print(f\"원본 데이터 크기: {len(df)}\")\n",
    "\n",
    "# sample_10k = df.sample(n=10000, random_state=42)  # 재현성을 위한 random_state\n",
    "# sample_1k = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# # CSV 파일로 저장\n",
    "# sample_10k.to_csv('./data/sentiment_10k_sample.csv', index=False)\n",
    "# sample_1k.to_csv('./data/train_1k.csv', index=False)\n",
    "\n",
    "# print(f\"10,000개 샘플 크기: {len(sample_10k)}\")\n",
    "# print(f\"1,000개 샘플 크기: {len(sample_1k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv('./data/train_1k.csv')\n",
    "\n",
    "# df = df[['comments','hate']]\n",
    "\n",
    "# hate_mapping = {\n",
    "#     'none': 0,      # 혐오 표현 없음\n",
    "#     'offensive': 1, # 공격적인 표현 (약한 혐오)\n",
    "#     'hate': 2       # 명백한 혐오 표현\n",
    "# }\n",
    "\n",
    "# # 매핑 적용\n",
    "# df['hate'] = df['hate'].map(hate_mapping)\n",
    "\n",
    "# df.to_csv('./data/hate.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 크기: 1000 행\n",
      "컬럼: ['comments', 'hate']\n",
      "데이터셋 임베딩 생성 중...\n",
      "임베딩 생성 완료: (1000, 384)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('./data/hate.csv')\n",
    "print(f\"데이터셋 크기: {len(df)} 행\")\n",
    "print(f\"컬럼: {df.columns.tolist()}\")\n",
    "\n",
    "# 임베딩 모델 로드\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 전체 데이터셋 임베딩 계산 및 저장\n",
    "print(\"데이터셋 임베딩 생성 중...\")\n",
    "all_embeddings = []\n",
    "for text in df['comments']:\n",
    "    embedding = model.encode(text)\n",
    "    all_embeddings.append(embedding)\n",
    "\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "print(f\"임베딩 생성 완료: {all_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 공격성 분석 시스템 =====\n",
      "텍스트를 입력하면 공격성 정도를 0-10점 척도로 분석합니다.\n",
      "\n",
      "===== 공격성 분석 결과 =====\n",
      "입력 텍스트: 이것도 못하냐 ㅋㅋ\n",
      "공격성 점수: 0.0/10\n",
      "공격성 수준: 낮음 (중립적 표현)\n",
      "\n",
      "----- 유사한 댓글 (참고) -----\n",
      "1. [원본: 2, 점수: 10/10] 양세형 빨갱이인줄...\n",
      "   (텍스트 점수: 0, 레이블 점수: 10, 유사도: 0.9579)\n",
      "2. [원본: 0, 점수: 0/10] 꿀잼 ㅠㅠ...\n",
      "   (텍스트 점수: 0, 레이블 점수: 0, 유사도: 0.9579)\n",
      "3. [원본: 1, 점수: 5/10] 똥싸고 자빠졌네...\n",
      "   (텍스트 점수: 0, 레이블 점수: 5, 유사도: 0.9579)\n",
      "4. [원본: 0, 점수: 0/10] 넘재밌었음강문영씨화이팅 김광규씨화이팅최성국시나오디마...\n",
      "   (텍스트 점수: 0, 레이블 점수: 0, 유사도: 0.9579)\n",
      "5. [원본: 2, 점수: 10/10] 얘가뭔죄고 맷퇘지들아ㅋㅋㅋ...\n",
      "   (텍스트 점수: 0, 레이블 점수: 10, 유사도: 0.9579)\n",
      "\n",
      "===== 공격성 분석 결과 =====\n",
      "입력 텍스트: 쓰래기야\n",
      "공격성 점수: 0.0/10\n",
      "공격성 수준: 낮음 (중립적 표현)\n",
      "\n",
      "----- 유사한 댓글 (참고) -----\n",
      "1. [원본: 2, 점수: 10/10] 양세형 빨갱이인줄...\n",
      "   (텍스트 점수: 0, 레이블 점수: 10, 유사도: 0.9021)\n",
      "2. [원본: 0, 점수: 0/10] 넘재밌었음강문영씨화이팅 김광규씨화이팅최성국시나오디마...\n",
      "   (텍스트 점수: 0, 레이블 점수: 0, 유사도: 0.9021)\n",
      "3. [원본: 2, 점수: 10/10] 얘가뭔죄고 맷퇘지들아ㅋㅋㅋ...\n",
      "   (텍스트 점수: 0, 레이블 점수: 10, 유사도: 0.9021)\n",
      "4. [원본: 1, 점수: 5/10] 똥싸고 자빠졌네...\n",
      "   (텍스트 점수: 0, 레이블 점수: 5, 유사도: 0.9021)\n",
      "5. [원본: 0, 점수: 0/10] 꿀잼 ㅠㅠ...\n",
      "   (텍스트 점수: 0, 레이블 점수: 0, 유사도: 0.9021)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# 공격성 분석 함수\n",
    "def analyze_aggression(query_text, top_k=10):\n",
    "    # 텍스트 기반 공격성 분석\n",
    "    offensive_word_scores = {\n",
    "        # 높은 수준의 혐오 표현 (8-10점)\n",
    "        \"씨발\": 10, \"시발\": 10, \"병신\": 9, \"지랄\": 9, \"개새끼\": 10, \"좆\": 9, \n",
    "        \"씹\": 8, \"년\": 8, \"놈\": 7, \"닥쳐\": 8, \"꺼져\": 8,\n",
    "        \n",
    "        # 중간 수준의 공격적 표현 (5-7점)\n",
    "        \"새끼\": 7, \"미친\": 6, \"죽어\": 7, \"죽여\": 9, \"바보\": 5, \"멍청이\": 6,\n",
    "        \"꺼지\": 6, \"뒤져\": 7, \"아가리\": 7, \"쓰레기\": 6, \"쓰렉\": 6,\n",
    "        \n",
    "        # 낮은 수준의 부정적 표현 (2-4점)\n",
    "        \"짜증\": 3, \"싫어\": 2, \"나쁜\": 3, \"못생긴\": 3, \"멍청한\": 4, \"못된\": 4\n",
    "    }\n",
    "    \n",
    "    # 텍스트에서 공격적 단어 찾기\n",
    "    text_lower = query_text.lower()\n",
    "    found_offensive_words = []\n",
    "    \n",
    "    for word, score in offensive_word_scores.items():\n",
    "        if word in text_lower:\n",
    "            found_offensive_words.append((word, score))\n",
    "    \n",
    "    # 최대 공격성 점수 계산\n",
    "    text_score = 0\n",
    "    if found_offensive_words:\n",
    "        text_score = max(score for _, score in found_offensive_words)\n",
    "    \n",
    "    # 쿼리 임베딩 계산\n",
    "    query_embed = model.encode(query_text)\n",
    "    \n",
    "    # 모든 댓글과의 유사도 계산\n",
    "    similarities = cosine_similarity([query_embed], all_embeddings)[0]\n",
    "    \n",
    "    # 유사도 내림차순으로 정렬된 인덱스\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # 유사한 댓글 정보 수집\n",
    "    similar_comments = []\n",
    "    for idx in top_indices:\n",
    "        comment_text = df.iloc[idx]['comments']\n",
    "        hate_value = df.iloc[idx]['hate']\n",
    "        similarity = similarities[idx]\n",
    "        \n",
    "        # 댓글 텍스트 분석으로 점수 추정\n",
    "        comment_score = 0\n",
    "        for word, score in offensive_word_scores.items():\n",
    "            if word in comment_text.lower():\n",
    "                comment_score = max(comment_score, score)\n",
    "        \n",
    "        # 'hate' 값을 0-10 스케일로 변환\n",
    "        hate_score = 0\n",
    "        if hate_value == 1:\n",
    "            hate_score = 5\n",
    "        elif hate_value == 2:\n",
    "            hate_score = 10\n",
    "            \n",
    "        # 최종 점수는 텍스트 분석과 레이블 중 높은 값\n",
    "        final_score = max(comment_score, hate_score)\n",
    "        \n",
    "        similar_comments.append({\n",
    "            'text': comment_text,\n",
    "            'hate_value': hate_value,\n",
    "            'display_score': final_score,\n",
    "            'text_score': comment_score,\n",
    "            'label_score': hate_score,\n",
    "            'similarity_score': similarity\n",
    "        })\n",
    "    \n",
    "    # 공격성 레벨 결정\n",
    "    if text_score < 3:\n",
    "        aggression_level = \"낮음 (중립적 표현)\"\n",
    "    elif text_score < 7:\n",
    "        aggression_level = \"중간 (약간 공격적)\"\n",
    "    else:\n",
    "        aggression_level = \"높음 (매우 공격적/혐오 표현)\"\n",
    "    \n",
    "    return {\n",
    "        'normalized_score': text_score,\n",
    "        'aggression_level': aggression_level,\n",
    "        'similar_comments': similar_comments,\n",
    "        'found_offensive_words': found_offensive_words\n",
    "    }\n",
    "\n",
    "# 메인 로직\n",
    "print(\"\\n===== 공격성 분석 시스템 =====\")\n",
    "print(\"텍스트를 입력하면 공격성 정도를 0-10점 척도로 분석합니다.\")\n",
    "\n",
    "while True:\n",
    "    query_text = input(\"\\n분석할 텍스트를 입력하세요 (종료하려면 'q' 입력): \")\n",
    "    \n",
    "    if query_text.lower() == 'q':\n",
    "        break\n",
    "    \n",
    "    # 분석 실행\n",
    "    result = analyze_aggression(query_text)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"\\n===== 공격성 분석 결과 =====\")\n",
    "    print(f\"입력 텍스트: {query_text}\")\n",
    "    print(f\"공격성 점수: {result['normalized_score']:.1f}/10\")\n",
    "    print(f\"공격성 수준: {result['aggression_level']}\")\n",
    "    \n",
    "    # 발견된 공격적 단어 출력\n",
    "    if result['found_offensive_words']:\n",
    "        print(\"\\n발견된 공격적 표현:\")\n",
    "        for word, score in result['found_offensive_words']:\n",
    "            print(f\"- '{word}' (점수: {score})\")\n",
    "    \n",
    "    print(\"\\n----- 유사한 댓글 (참고) -----\")\n",
    "    for i, comment in enumerate(result['similar_comments'][:5], 1):\n",
    "        print(f\"{i}. [원본: {comment['hate_value']}, 점수: {comment['display_score']}/10] {comment['text'][:100]}...\")\n",
    "        print(f\"   (텍스트 점수: {comment['text_score']}, 레이블 점수: {comment['label_score']}, 유사도: {comment['similarity_score']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
