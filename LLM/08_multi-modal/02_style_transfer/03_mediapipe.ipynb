{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742863133.626802   55164 gl_context.cc:369] GL version: 2.1 (2.1 INTEL-23.0.22), renderer: Intel(R) Iris(TM) Plus Graphics 655\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# 얼굴 검출 모델 초기화\n",
    "mp_face_detector = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "face_detector = mp_face_detector.FaceDetection(model_selection=0, min_detection_confidence=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 얼굴, 눈, 코, 입, 귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1742863133.670548   55784 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2025-03-25 09:38:54.115 python[6342:55164] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "2025-03-25 09:38:55.685 python[6342:55164] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-25 09:38:55.685 python[6342:55164] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 얼굴 영역을 찾을 수 있는 함수\n",
    "    results = face_detector.process(img_rgb)\n",
    "\n",
    "    # print(results.detections)\n",
    "\n",
    "    if results.detections:\n",
    "        for detection in  results.detections:\n",
    "            mp_drawing.draw_detection(frame, detection)\n",
    "\n",
    "    cv2.imshow('Face Detectinon', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 얼굴 인식 모자이크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w, _ =img_rgb.shape\n",
    "\n",
    "    # 얼굴 영역을 찾을 수 있는 함수\n",
    "    results = face_detector.process(img_rgb)\n",
    "\n",
    "    if results.detections:\n",
    "        for detection in  results.detections:\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "\n",
    "            x1 = int(max(bbox.xmin, 0) * img_w)\n",
    "            y1 = int(max(bbox.ymin, 0) * img_h)\n",
    "            width = int(bbox.width * img_w)\n",
    "            height = int(bbox.height * img_h)\n",
    "            x2 = min(x1 + width, img_w)\n",
    "            y2 = min(y1 + height, img_h)\n",
    "\n",
    "            face = img_rgb[y1:y2, x1:x2]\n",
    "            face = cv2.resize(face, (10, 10), interpolation=cv2.INTER_LINEAR)\n",
    "            face = cv2.resize(face, (x2-x1, y2-y1), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            img_rgb[y1:y2, x1:x2] = face\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('Face Detection', img_rgb)  \n",
    "    \n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.destroyWindow('Face Detection') \n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. FaceMesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742863495.816467   55164 gl_context.cc:369] GL version: 2.1 (2.1 INTEL-23.0.22), renderer: Intel(R) Iris(TM) Plus Graphics 655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1742863495.827797   61593 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742863495.856009   61593 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# 얼굴 인식 및 얼굴 메쉬 솔루션 위한 초기화\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742865506.927027   55164 gl_context.cc:369] GL version: 2.1 (2.1 INTEL-23.0.22), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "W0000 00:00:1742865506.933878   92565 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742865506.964074   92568 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as face_mesh:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        results = face_mesh.process(image)\n",
    "        image.flags.writeable = True\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmark in results.multi_face_landmarks:\n",
    "\n",
    "                # 얼굴 윤곽선 강조\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmark,\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()    \n",
    "                )\n",
    "\n",
    "                # # 얼굴 윤곽선과 주요 특징 강조 ( 윤곽선, 눈썹, 눈, 입술)\n",
    "                # mp_drawing.draw_landmarks(\n",
    "                #     image=image,\n",
    "                #     landmark_list=face_landmark,\n",
    "                #     connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                #     landmark_drawing_spec=None,\n",
    "                #     connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style()    \n",
    "                # )\n",
    "\n",
    "                # # 눈의 형태와 위치 강조\n",
    "                # mp_drawing.draw_landmarks(\n",
    "                #     image=image,\n",
    "                #     landmark_list=face_landmark,\n",
    "                #     connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                #     landmark_drawing_spec=None,\n",
    "                #     connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style()    \n",
    "                # )\n",
    "\n",
    "        cv2.imshow('Face mesh', image)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 필터 씌우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742866165.264066  103587 gl_context.cc:369] GL version: 2.1 (2.1 INTEL-23.0.22), renderer: Intel(R) Iris(TM) Plus Graphics 655\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1742866165.285398  103678 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742866165.307316  103678 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mask_img = cv2.imread('jeungyoun.png', cv2.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 얼굴 위에 오버레이 이미지를 합성하는 함수\n",
    "def face_overlay(background_img, overlay_img, x, y, overlay_size=None):\n",
    "    try:\n",
    "        bg_img = background_img.copy()\n",
    "\n",
    "        if bg_img.shape[2] == 3:\n",
    "            bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGR2BGRA)\n",
    "        \n",
    "        if overlay_size is not None:\n",
    "            overlay_img = cv2.resize(overlay_img.copy(), overlay_size)\n",
    "\n",
    "        b, g, r, a = cv2.split(overlay_img)\n",
    "        mask = cv2.medianBlur(a, 5)\n",
    "\n",
    "        h, w, _ = overlay_img.shape\n",
    "        roi = bg_img[int(y - h/2): int(y + h/2), int(x - w/2):int(x + w/2)]\n",
    "        img_bg1 = cv2.bitwise_and(roi, roi, mask=cv2.bitwise_not(mask))\n",
    "        img_bg2 = cv2.bitwise_and(overlay_img.copy(), overlay_img.copy(), mask=mask)\n",
    "        bg_img[int(y - h/2): int(y + h/2), int(x - w/2):int(x + w/2)] = cv2.add(img_bg1, img_bg2)\n",
    "        \n",
    "        bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGRA2BGR)\n",
    "        return bg_img\n",
    "\n",
    "    except:\n",
    "        return background_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(img_rgb)\n",
    "    img_h, img_w, img_c = img.shape\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "        for face_landmark in result.multi_face_landmarks:\n",
    "            xy_point = []\n",
    "\n",
    "            for c, lm in enumerate(face_landmark.landmark):\n",
    "                xy_point.append([lm.x, lm.y])\n",
    "                img = cv2.circle(img, (int(lm.x * img_w), int(lm.y * img_h)), 1, (255,0,0), 2)\n",
    "            \n",
    "            top_left = np.min(xy_point, axis=0)\n",
    "            bottom_right = np.max(xy_point, axis=0)\n",
    "            mean_xy = np.mean(xy_point, axis=0)\n",
    "            img = cv2.circle(img, (int(mean_xy[0] * img_w), int(mean_xy[1] * img_h)), 4, (255,0,0), 3)\n",
    "            face_width = int(bottom_right[0] * img_w) - int(top_left[0] * img_w)\n",
    "            face_height = int(bottom_right[1] * img_h) - int(top_left[1] * img_h)\n",
    "            \n",
    "            if face_width > 0 and face_height > 0:\n",
    "                face_result = face_overlay(\n",
    "                    img, \n",
    "                    mask_img, \n",
    "                    int(mean_xy[0] * img_w), \n",
    "                    int(mean_xy[1] * img_h), \n",
    "                    overlay_size=(face_width, face_height)\n",
    "                )\n",
    "\n",
    "    else:\n",
    "        face_result = img\n",
    "\n",
    "    cv2.imshow('mini mouse', face_result)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
